
# 模型框架：预训练---->以bert为基础训练差异化模型---->使用stacking进行模型集成

## 语言模型预训练

- 在一开始的实验中我们发现，百度的ernie会比bert_chinese_base效果好，经过查阅资料，我们得知果用同domain相近的语料进行语言模型预训练，可以提升模型效果
，因此我们先后用ernie在训练集、验证集、网上搜索的微博语料、以及最终的测试集上进行了语言模型预训练，后续实验表明预训练确实会带来一定的提升。

## 半监督学习

+ 实验过程中我们发现模型在训练集上模型发生了严重拟合，在训练集上的准确率达到97%，加上通过分析错误样例，发现模型容易被个词语所误导，而不是关心整个句子本身，因此我们开始了增加模型泛化性的实验。受到[Unsupervised Data Augmentation](https://github.com/google-research/uda)的启发，我们使用了tsa和vat两项半监督学习技术来增强模型的泛化能力。

+ ### VAT
    
    - 在训练过程中，以20%的机率mask掉一些字，然后计算扰动后的句子输出和没有扰动句子的输出之间的kl上散度，将它加入到原本的loss中作为最终的loss，以此增强模型的抗干扰能力。

+ ### TSA
    
    - 设定一个变动的阈值，在每一个batch当中，如果一个example能被正确预测，并且预测概率超过了设定阈值，那么在loss回传并更新参数时，这个example就不被纳入计算。这个方法能让模型更多的在划分错误的example上进行学习。

+ 实验表明使用这两种技术确实会给模型带来提升，在accuracy这项指标上可以得到1%左右的提升。


## 模型多样性

+ **到目前为止，使用以上方法，只用bert+线性层，已经能在验证集上达到84%的准确率，如果换成bilstm+attention，则可以达到84.6%，此时比赛也已经到了尾声，我们在单模型上已经没有提升的空间；进入模型集成阶段，我们使用的策略是训练***尽可能不一样***的模型来减少模型输出间的相关性，从而增加集成的性能**。

  - bert和不同模型的堆叠:我们分别使用了bert+线性层和bert+bilstm+attention。

  - max_length：实验过程中发现max_length会影响表现，最后我们分别使用120，160，200的长度来训练模型。

  - 上下文：使用不同上下文长度以及不同的上下文机制。

  - 增加类别的权重：实验中我们发现积极类别的召回率低很多，因此我们又训练了一个在积极类别上表现比较好的模型，方法在计算loss的时候增加积极类别的权重。
