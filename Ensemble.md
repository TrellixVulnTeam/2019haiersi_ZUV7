### 模型融合

- 最后提交的版本一共使用了 7 个 bert 模型，采用 stacking 进行集成 。
- stacking 流程：标准的 stacking 流程是，先对模型进行 k 折训练，将 k 折对验证集的预测结果做拼接作为二层训练集，对测试集的预测结果做平均作为二层测试集。由于提交时间的限制，我们用原始的训练集和验证集拼接之后 k 折训练模型，再将结果按 7:3 划分成二层训练集与测试集。二层学习器采用随机森林，经 GridSearch选择超参后，重新在整个训练集加验证集上训练二层学习器，应用到最终的测试集上。
- 模型的选择：除了 bert 模型以外，我们也尝试过融合一些基于传统机器学习的模型，例如以 TF-IDF 编码的 SVM，GBDT，XGB，RF、以 word-embedding * TF-IDF 编码的 SVM，GBDT，XGB，RF、以 word-embedding 编码的 rnn 模型，经过测试以后这些模型与单个 bert 模型融合都能提升 1-2% 的准确率，但效果不如 2 个 bert 或者多个 bert 的组合，最终没有将这些模型进行融合。
