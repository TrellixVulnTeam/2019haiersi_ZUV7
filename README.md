
# 模型框架：预训练---->以bert为基础训练差异化模型---->使用stacking进行模型集成

## 语言模型预训练

- 在一开始的实验中我们发现，百度的ernie会比bert_chinese_base效果好，经过查阅资料，我们得知果用同domain相近的语料进行语言模型预训练，可以提升模型效果
，因此我们先后用ernie在训练集、验证集、网上搜索的微博语料、以及最终的测试集上进行了语言模型预训练，后续实验表明预训练确实会带来一定的提升。

## 使用上下文

  + 在这个数据集上，我们被要求，对若干个句子之中的特定一句输出其隐式情感。非常自然的，我们假定目标句的上下文为目标句提供了额外信息，并且基于这个假想进行了一些实验。

- ### **朴素拼接**

  + 我们尝试将目标句，以及其上下文进行拼接，作为一个更大的整体来判断情感。但是，在几次实验当中，这个方案效果不大，并且有时有着比仅使用目标句更为糟糕的表现。通过观察错误样例，我们认为这可能是因为邻近句子并不总是和目标句保持相近的情感所导致的。

- ### **test_a和test_b**

  + 受到了[Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence](https://www.aclweb.org/anthology/N19-1035)的启发，我们也尝试了一种类似于构造句子的做法。我们将拼接了上下文的长句子作为test_a，将目标句作为test_b，期待能达到询问*在该上下文中，目标句的情感是什么*的效果。相较于仅使用目标句，该方法在恰当的参数设置下，在dev集上能提高1%左右的正确率。

- ### **添加前缀后缀的多句子拼接**

  + 不加任何分隔符的拼接方案，在bert模型上的实现，终究是只取出最后一层隐状态的第一个位置，进行后续处理得到预测的分类。从bert的模型设计而言，该向量表达的是输入序列整体上的分类倾向，而不是一段上下文之中，我们所要的目标句的分类倾向。

  + 为了解决这个问题，我们参考了[Fine-tune BERT for Extractive Summarization](https://arxiv.org/abs/1903.10318)的做法，在每个句子的前后分别添上\[CLS]和\[SEP]标记，然后再进行拼接。在计算分类时，取出目标的\[CLS]对应的向量来计算。相较于test_a+test_b的方法，在dev集上能再提升约0.7%正确率。


## 半监督学习

+ 实验过程中我们发现模型在训练集上模型发生了严重拟合，在训练集上的准确率达到97%，加上通过分析错误样例，发现模型容易被个词语所误导，而不是关心整个句子本身，因此我们开始了增加模型泛化性的实验。受到[Unsupervised Data Augmentation](https://github.com/google-research/uda)的启发，我们使用了tsa和vat两项半监督学习技术来增强模型的泛化能力。

+ ### VAT
    
    - 在训练过程中，以20%的机率mask掉一些字，然后计算扰动后的句子输出和没有扰动句子的输出之间的kl上散度，将它加入到原本的loss中作为最终的loss，以此增强模型的抗干扰能力。

+ ### TSA
    
    - 设定一个变动的阈值，在每一个batch当中，如果一个example能被正确预测，并且预测概率超过了设定阈值，那么在loss回传并更新参数时，这个example就不被纳入计算。这个方法能让模型更多的在划分错误的example上进行学习。

+ 实验表明使用这两种技术确实会给模型带来提升，在accuracy这项指标上可以得到1%左右的提升。


## 模型多样性

+ **到目前为止，使用以上方法，只用bert+线性层，已经能在验证集上达到84%的准确率，如果换成bilstm+attention，则可以达到84.6%，此时比赛也已经到了尾声，我们在单模型上已经没有提升的空间；进入模型集成阶段，我们使用的策略是训练***尽可能不一样***的模型来减少模型输出间的相关性，从而增加集成的性能**。

  - bert和不同模型的堆叠:我们分别使用了bert+线性层和bert+bilstm+attention。

  - max_length：实验过程中发现max_length会影响表现，最后我们分别使用120，160，200的长度来训练模型。

  - 上下文：使用不同上下文长度以及不同的上下文机制。

  - 增加类别的权重：实验中我们发现积极类别的召回率低很多，因此我们又训练了一个在积极类别上表现比较好的模型，方法在计算loss的时候增加积极类别的权重。
 
 ## 模型融合

- 最后提交的版本一共使用了 7 个 bert 模型，采用 stacking 进行集成 。
- stacking 流程：标准的 stacking 流程是，先对模型进行 k 折训练，将 k 折对验证集的预测结果做拼接作为二层训练集，对测试集的预测结果做平均作为二层测试集。由于提交时间的限制，我们用原始的训练集和验证集拼接之后 k 折训练模型，再将结果按 7:3 划分成二层训练集与测试集。二层学习器采用随机森林，经 GridSearch选择超参后，重新在整个训练集加验证集上训练二层学习器，应用到最终的测试集上。
- 模型的选择：除了 bert 模型以外，我们也尝试过融合一些基于传统机器学习的模型，例如以 TF-IDF 编码的 SVM，GBDT，XGB，RF、以 word-embedding * TF-IDF 编码的 SVM，GBDT，XGB，RF、以 word-embedding 编码的 rnn 模型，经过测试以后这些模型与单个 bert 模型融合都能提升 1-2% 的准确率，但效果不如 2 个 bert 或者多个 bert 的组合，最终没有将这些模型进行融合。
